{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/49Hx7Ic8RCZR33reyHAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/SCAREDUnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmm3BXrG5vEk",
        "outputId": "3a80609a-6808-41d8-9399-81d52a3cc0ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##read data"
      ],
      "metadata": {
        "id": "9RnFl1MByjBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1mVumZGRmJ0ZybH1IITGQ-q_M0xvvqvw8'\n",
        "#url = \"https://drive.google.com/file/d/1mVumZGRmJ0ZybH1IITGQ-q_M0xvvqvw8/view?usp=share_link\"\n",
        "gdown.download(url,'SCARED.zip',quiet=True) \n",
        "import os\n",
        "if not os.path.exists('/content/SCARED/'):\n",
        "  !unzip -q SCARED.zip "
      ],
      "metadata": {
        "id": "HMoDyTartLs6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rm SCARED.zip"
      ],
      "metadata": {
        "id": "beQnZybxuQSj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cXMNFtutqPYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bcc11c-bae3-4d74-84c0-c2553fe1517c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "sys.path.append('/content/drive/') \n",
        "\n",
        "import os\n",
        "if not os.path.exists('/content/SCARED/'):\n",
        "  !unzip -q /content/drive/MyDrive/SCARED.zip "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/7569553/working-with-tiffs-import-export-in-python-using-numpy\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "import pandas as pd\n",
        "\n",
        "# gtpath = \"/content/GT/\"\n",
        "# if path.exists(gtpath) == False:\n",
        "#   os.mkdir(gtpath)\n",
        "\n",
        "path = \"/content/SCARED/\"\n",
        "filelist = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "#         filelist.append(os.path.join(root, file))   #cancatenate folder name with file name\n",
        "    filelist.append(file)\n",
        "filelist.sort()\n",
        "data = {\n",
        "    \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
        "    \"depth\": [x for x in filelist if x.endswith(\".tiff\")]\n",
        "}\n",
        "print(\"dataset size: \", len(data[\"image\"]), len(data[\"depth\"]))\n",
        "df = pd.DataFrame(data)\n",
        "df = df.sample(frac=1, random_state=1)#A random 100% sample, reproducible 1\n",
        "train = df[:42].reset_index(drop=\"true\")#avoid old index is added as a column, only replace old index with new sequential index is used\n",
        "valid = df[42:].reset_index(drop=\"true\")\n",
        "print(\"table of file names of validation set : \\n\",valid)\n",
        "#print(valid.at[0,\"depth\"], valid[\"depth\"][0])  #the same"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dMZdoPfrwnm",
        "outputId": "897c86fd-4d0b-4631-d7dc-6e5b2192c348"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size:  48 48\n",
            "table of file names of validation set : \n",
            "                   image                      depth\n",
            "0  d2k1_Right_Image.png  d2k1_right_depth_map.tiff\n",
            "1  d1k5_Right_Image.png  d1k5_right_depth_map.tiff\n",
            "2   d1k5_Left_Image.png   d1k5_left_depth_map.tiff\n",
            "3   d2k2_Left_Image.png   d2k2_left_depth_map.tiff\n",
            "4  d5k2_Right_Image.png  d5k2_right_depth_map.tiff\n",
            "5  d4k4_Right_Image.png  d4k4_right_depth_map.tiff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prepare dataset"
      ],
      "metadata": {
        "id": "v22m8xNLynBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/qiaofengsheng/pytorch-UNet/blob/e9d2ec1f6f7c9336093e71e62b68c1adf5ecdbe0/data.py#L14\n",
        "from PIL import Image\n",
        "import tifffile\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "DIR = \"/content/SCARED/\"\n",
        "def keep_image_size_open(path, size=(128, 128)):\n",
        "    img = Image.open(path)\n",
        "    temp = max(img.size)\n",
        "    mask = Image.new('P', (temp, temp))\n",
        "    mask.paste(img, (0, 0))\n",
        "    mask = mask.resize(size)\n",
        "    return mask\n",
        "def keep_image_size_open_tiff(path, size=(128, 128)):\n",
        "    img = tifffile.imread(DIR + path)[:,:,2]\n",
        "    img = np.nan_to_num(img, posinf=0, neginf=0)\n",
        "    colorimg = cv2.applyColorMap(cv2.convertScaleAbs(img, alpha=2.5), cv2.COLORMAP_JET)\n",
        "    colorimg = Image.fromarray(colorimg)\n",
        "    temp = max(colorimg.size)\n",
        "    tiff = Image.new('RGB', (temp, temp))#'P' if grayscale\n",
        "    tiff.paste(colorimg, (0, 0))\n",
        "    tiff = tiff.resize(size)\n",
        "    return tiff\n",
        "def keep_image_size_open_rgb(path, size=(128, 128)):\n",
        "    img = Image.open(DIR + path)\n",
        "    temp = max(img.size)\n",
        "    image = Image.new('RGB', (temp, temp))\n",
        "    image.paste(img, (0, 0))\n",
        "    image = image.resize(size)\n",
        "    return image\n",
        "\n",
        "print(len(df),len(train), len(valid))\n",
        "keep_image_size_open_rgb(train[\"image\"][4]) # ,(1280,1280) if enough RAM\n",
        "square_tiff = keep_image_size_open_tiff(train[\"depth\"][4] )\n",
        "#torch.Tensor(np.array(square_tiff))\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "#from utils import *\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    depth = keep_image_size_open_tiff(self.data[\"depth\"][i],(32,32))# ,(1280,1280) if enough RAM\n",
        "    image = keep_image_size_open_rgb(self.data[\"image\"][i],(32,32))\n",
        "    return transform(image), torch.Tensor(np.array(depth)) #np.array(image), np.array(depth)\n",
        "\n",
        "\n",
        "from torch.nn.functional import one_hot\n",
        "d = MyDataset(train)\n",
        "print(d[0][0].shape)\n",
        "print(np.transpose(d[0][0],(1,2,0)).shape)\n",
        "print(d[0][1].shape)\n",
        "print(d[0][1].long().shape)\n",
        "print(one_hot(d[0][1].long()).shape)\n",
        "\n",
        "if d[0][0].shape == (3, 1280, 1280):\n",
        "  d[0][0] = np.transpose(d[0][0],(1,2,0))\n",
        "  print(\"transposed!\")\n",
        "plt.figure(figsize=(15,9))\n",
        "plt.subplot(121)\n",
        "plt.imshow(d[0][0])\n",
        "plt.subplot(122)\n",
        "plt.imshow(d[0][1])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "vlbC_YfsfyhK",
        "outputId": "336bdf93-6bdd-4c0b-d203-6bf7715361fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 42 6\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([32, 32, 3])\n",
            "torch.Size([32, 32, 3])\n",
            "torch.Size([32, 32, 3])\n",
            "torch.Size([32, 32, 3, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5f84f684c591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 32, 32) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##net"
      ],
      "metadata": {
        "id": "UZOXn4phytwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://zhuanlan.zhihu.com/p/467623152\n",
        "#https://github.com/milesial/Pytorch-UNet\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "QwxTHo2HC3jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        out_channels = [4,8,16,32,64]\n",
        "        #out_channels = [16,32,64,128,256]\n",
        "        #out_channels = [40,80,160,320,640]\n",
        "        #out_channels = [64,128,256,512,1024]\n",
        "        #out_channels = [80,160,320,640,1280]\n",
        "        self.inc = DoubleConv(n_channels, out_channels[0])\n",
        "        self.down1 = Down(out_channels[0], out_channels[1])\n",
        "        self.down2 = Down(out_channels[1], out_channels[2])\n",
        "        self.down3 = Down(out_channels[2], out_channels[3])\n",
        "        self.down4 = Down(out_channels[3], out_channels[4])\n",
        "        self.up1 = Up(out_channels[4], out_channels[3])\n",
        "        self.up2 = Up(out_channels[3], out_channels[2])\n",
        "        self.up3 = Up(out_channels[2], out_channels[1])\n",
        "        self.up4 = Up(out_channels[1], out_channels[0])\n",
        "        self.outc = OutConv(out_channels[0], n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "TqjjPWi1P9-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##training"
      ],
      "metadata": {
        "id": "c7z4zPr1yyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tqdm\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "weight_path = 'params/unet.pth'\n",
        "# data_path = r'data'\n",
        "save_path = 'train_image'\n",
        "\n",
        "\n",
        "num_classes = 3\n",
        "data_loader = DataLoader(MyDataset(train), batch_size=1, shuffle=True)\n",
        "net = UNet(3,num_classes).to(device)\n",
        "# if os.path.exists(weight_path):\n",
        "#   net.load_state_dict(torch.load(weight_path))\n",
        "#   print('successful load weightÔºÅ')\n",
        "# else:\n",
        "#   print('not successful load weight')\n",
        "\n",
        "!pip -q install torchmetrics\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "def ssim(preds, target):\n",
        "  ssim_loss = StructuralSimilarityIndexMeasure().to(device)(preds, target)\n",
        "  ssim_loss = torch.mean(1 - ssim_loss)\n",
        "  return ssim_loss\n",
        "\n",
        "opt = optim.Adam(net.parameters())\n",
        "#loss_fun = nn.CrossEntropyLoss()#ssim()\n",
        "\n",
        "\n",
        "epoch = 1\n",
        "while epoch < 10:\n",
        "  for i, (image, depth) in enumerate(tqdm.tqdm(data_loader)):\n",
        "    depth = np.transpose(depth, (0,3,1,2))#[:,2,:,:]# depth = torch.squeeze(np.transpose(depth, (0,3,1,2), 1)\n",
        "    image, depth = image.to(device), depth.to(device)#torch.Size([1, 3, 1280, 1280])\n",
        "    pred = net(image)   #expected input[1, 1280, 1280, 3] to have 3 channels\n",
        "    #print(image.shape, pred.shape, depth.long().shape)\n",
        "    #torch.Size([1, 3, 1280, 1280]) torch.Size([1, 1, 1280, 1280]) torch.Size([1, 1280, 1280, 3])\n",
        "    #torch.Size([1, 3, 1280, 1280]) torch.Size([1, 1, 1280, 1280]) torch.Size([1, 3, 1280, 1280]) depth transposed\n",
        "    #torch.Size([1, 3, 1280, 1280]) torch.Size([1, 1, 1280, 1280]) torch.Size([1, 1280, 1280]) depth transposed and second dim droped\n",
        "    train_loss = loss_fun = ssim(pred, depth.long().type(torch.float32))#Expected input batch_size (1) to match target batch_size (3).\n",
        "    # input pre:(Class), (Nbatch, Class) or (N,C,d_1,d_2,...)\n",
        "    # target depth:(), (Nbatch,) or (N,d_1,d_2,...)\n",
        "    opt.zero_grad()\n",
        "    train_loss.backward()\n",
        "    opt.step()\n",
        "    if i % 20 == 0:\n",
        "        print(f'{epoch}-{i}-train_loss===>>{train_loss.item()}')\n",
        "  # if epoch % 10 == 0:\n",
        "  #   torch.save(net.state_dict(), weight_path)\n",
        "  #   print('save successfully!')\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "9_BzUTT6zYV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import torch\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import torch.optim as optim\n",
        "# import collections\n",
        "# import torch.nn as nn\n",
        "\n",
        "# #test example\n",
        "# input, target = next(iter(SCAREDDataset(data=valid, batch_size=2)))\n",
        "# plt.figure(figsize=(15,9))\n",
        "# for i, j in enumerate([0,3]):\n",
        "#   plt.subplot(231+j)\n",
        "#   plt.imshow((input[i] / 255))\n",
        "#   plt.subplot(232+j)\n",
        "#   plt.imshow(target[i])\n",
        "#   plt.subplot(233+j)\n",
        "#   plt.imshow(cv2.applyColorMap(cv2.convertScaleAbs(target[i], alpha=7.4), cv2.COLORMAP_JET))\n",
        "# # fig, ax = plt.subplots(2, 2, figsize=(150, 150))\n",
        "# # for i in range(2):\n",
        "# #   ax[i, 0].imshow((input[i]))\n",
        "# #   ax[i, 1].imshow((target[i]), cmap=plt.cm.jet.set_bad(color=\"black\"))\n",
        "\n",
        "# # dataloaders = {\n",
        "# #   'train':  SCAREDDataset(data=train, batch_size=2),\n",
        "# #   'val': SCAREDDataset(data=valid, batch_size=2)\n",
        "# # }\n",
        "# # images, depthmaps = dataloaders[\"val\"]\n",
        "# # for image in images:\n",
        "# #   image = np.transpose(image,(0,3,1,2))\n",
        "\n",
        "# # for images, depthmaps in dataloaders['val']:\n",
        "# #   #for image in images:#(2, 1024, 1280, 3)# channels-last\n",
        "# #     images = np.transpose(images,(0,3,1,2))\n",
        "\n",
        "# training_set = SCAREDDataset(data=train, batch_size=2)\n",
        "# training_iter = iter(SCAREDDataset(data=train, batch_size=2))\n",
        "# batch = next(training_iter)\n",
        "# images, depthmaps = batch\n",
        "# print(training_set, training_iter)  #SCAREDDataset #Iteraotr\n",
        "# #print(batch)#tuple 4d array\n",
        "# print(images.shape, depthmaps.shape)#array  #(2, 1024, 1280, 3) (2, 1024, 1280, 3)\n",
        "# print(images[0].shape)#(1024, 1280, 3)\n",
        "\n",
        "\n",
        "# #training_set, validation_set = next(iter(SCAREDDataset(data=train, batch_size=2))), next(iter(SCAREDDataset(data=valid, batch_size=2)))\n",
        "# model  = UNet(3, 1)#first in_channel, n_classes  \n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# final_losses=[]\n",
        "# t_start = time.time()\n",
        "# for i in range(3):\n",
        "#   model.train(mode=True)\n",
        "#   batch = next(training_iter) \n",
        "#   images, depthmaps = batch\n",
        "#   images = np.transpose(images,(0,3,1,2))\n",
        "#   depthmaps = np.transpose(depthmaps, (0,3,1,2))\n",
        "#   print(depthmaps.shape)#(2, 3, 1024, 1280)\n",
        "#   pred = model(images)\n",
        "#   loss = loss(pred, depthmaps)\n",
        "#   final_losses.apend(loss)\n",
        "#   print(\"epoch: {}  loss: {}\".format(i,loss.item()))\n",
        "#   optimizer.zero_grad()\n",
        "#   loss.backward() \n",
        "#   optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "R2q4eHKAub2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.ones(5)\n",
        "# b = a.copy()\n",
        "# print(a)\n",
        "# for i, j in enumerate([4,5]):\n",
        "#   a[i,] = j\n",
        "#   b[i] = j\n",
        "#   print(i,j)\n",
        "\n",
        "#   print(a,b,a==b)"
      ],
      "metadata": {
        "id": "cAaEXpHg6njY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# x = np.array([[[0], [1], [2]]])\n",
        "# print(x.shape,\n",
        "# np.squeeze(x, axis=-1).shape,\n",
        "# np.squeeze(x, axis=-1))\n",
        "\n",
        "# i = np.empty((2, *(1,2), 3))#4D numpy array\n",
        "# d = np.empty((2, *(1,2), 1))\n",
        "# print(d)\n"
      ],
      "metadata": {
        "id": "Jt_8GbYe5EuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# # The inputs are 1024x1280 RGB images with 3 `channels_last` and the batch size is 2.\n",
        "# batch_size = 2\n",
        "# in_channel = 3\n",
        "# height, width = 1024, 1280\n",
        "# out_channel = 320\n",
        "# kernal_size = 3\n",
        "# input_shape = (batch_size, height, width, in_channel)#batch size, height, width, channel\n",
        "# x = tf.random.normal(input_shape)\n",
        "\n",
        "# y = tf.keras.layers.Conv2D(out_channel, kernal_size, padding=\"same\")(x)# With strides=(1, 1), `padding` as \"same\".no activation\n",
        "# print(y.shape)  #(2, 1024, 1280, 320)\n",
        "\n",
        "# y = tf.keras.layers.MaxPool2D(strides=(2, 2))(x)# strides=(2, 2)\n",
        "# print(y.shape)  #(2, 512, 640, 3)"
      ],
      "metadata": {
        "id": "N8xPuNrC_WO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# import torch\n",
        "\n",
        "# # The inputs are 1024x1280 RGB images with 3 `channels_first` and the batch size is 2.\n",
        "# batch_size = 2\n",
        "# in_channel = 3\n",
        "# height, width = 1024, 1280\n",
        "# out_channel = 320\n",
        "# kernal_size = 3\n",
        "# input_size = (batch_size, in_channel, height, width) #batch size, channels_first, height, width\n",
        "# x = torch.randn(input_size)#x = torch.normal(2, 3, input_shape)#mean, std, size \n",
        "\n",
        "# m = nn.Conv2d(in_channel, out_channel, kernal_size, padding=1)(x)# square kernels and with padding\n",
        "# print(m.shape)#torch.Size([2, 320, 1024, 1280])\n",
        "\n",
        "# m = nn.MaxPool2d(batch_size)(x)# square kernels and with padding default stride=batch_size\n",
        "# print(m.shape)#torch.Size([2, 3, 512, 640])"
      ],
      "metadata": {
        "id": "G5pK2APhP8FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_config():\n",
        "#   return dict(\n",
        "#       net = 'unet',\n",
        "#       encoder = dict(),\n",
        "#       data = 'scared', \n",
        "#       net_config = dict(),\n",
        "#       training=dict(\n",
        "#             batch_size=12,\n",
        "#             learning_rate=1e-4,\n",
        "#             epochs=3,\n",
        "#             mode='pair',\n",
        "#             loss='ssim',  # other: hamming\n",
        "#             n_training_steps=500000),  # control time\n",
        "#       evaluation=dict(batch_size=2),\n",
        "#       seed = 8\n",
        "#   )\n",
        "# config = get_config()\n",
        "# for (k, v) in config.items():\n",
        "#   print(\"{} = {}\".format(k, v))#print(\"%s = %s\" % (k, v))#print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "id": "42ZMqznMf9Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = config['seed']\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "torch.manual_seed(seed + 2)"
      ],
      "metadata": {
        "id": "8znGKLUWgFvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tifffile\n",
        "# import cv2\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# print(\"image shape: {}  depth shape: {}\".format(cv2.imread(path + df.at[37,\"image\"]).shape, \n",
        "#                                                  tifffile.imread(path + df.at[43,\"depth\"]).shape))\n",
        "# #WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
        "# image_example1 = cv2.imread(path + df.at[37,\"image\"])\n",
        "# image_example2 = cv2.imread(path + df.at[43,\"image\"])\n",
        "# depth_example = tifffile.imread(path + df.at[43,\"depth\"])[:,:,2].astype(np.float32) * 256.0\n",
        "# depth_example2 = tifffile.imread(path + df.at[43,\"depth\"]).squeeze().astype(np.float32) \n",
        "# colored_depth_example = cv2.applyColorMap(cv2.convertScaleAbs(depth_example, alpha=0.015), cv2.COLORMAP_JET)\n",
        "# colored_depth_example2 = cv2.applyColorMap(cv2.convertScaleAbs(depth_example2, alpha=7.4), cv2.COLORMAP_JET)\n",
        "# print(\"image shape: {}  depth shape: {} / {}\".format(image_example1.shape, depth_example.shape, depth_example2.shape))\n",
        "# plt.figure(figsize=(15,9))\n",
        "# for i in range(0,6,3):\n",
        "#   plt.subplot(231+i)\n",
        "#   plt.imshow(image_example2)\n",
        "#   #plt.imshow(depth_example2)\n",
        "#   plt.subplot(232+i)\n",
        "#   plt.imshow(colored_depth_example)\n",
        "#   plt.subplot(233+i)\n",
        "#   plt.imshow(colored_depth_example2)"
      ],
      "metadata": {
        "id": "cwIr1EzWDSyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tifffile\n",
        "# import numpy as np\n",
        "# from torch.utils.data import Dataset\n",
        "# import cv2\n",
        "\n",
        "# HEIGHT = 1024\n",
        "# WIDTH = 1280\n",
        "\n",
        "# class SCAREDDataset(Dataset):\n",
        "#   def __init__(self, data, batch_size, dim=(HEIGHT,WIDTH), shuffle=True):\n",
        "#     self.data = data\n",
        "#     self.batch_size = batch_size\n",
        "#     self.dim = dim\n",
        "#     self.id = np.arange(len(data[\"image\"]))\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return int(len(self.data))//self.batch_size\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     #TODO: if (index+1) > len(data)\n",
        "#     idx = self.id[index*self.batch_size : (index+1)*self.batch_size]\n",
        "#     batch = [self.id[i] for i in idx]\n",
        "#     image, depth = self.generation(batch)\n",
        "#     return image, depth\n",
        "\n",
        "#   def generation(self, batch):\n",
        "#     image = np.empty((self.batch_size, *self.dim, 3))\n",
        "#     depth = np.empty((self.batch_size, *self.dim, 3))\n",
        "#     for i, j in enumerate(batch):\n",
        "#       image[i] = cv2.imread(path+self.data.at[j,\"image\"])\n",
        "#       depth[i] = tifffile.imread(path+self.data.at[j,\"depth\"]).astype(np.float32).squeeze()#or [:,:,2]\n",
        "#     return image, depth  \n",
        "#     #TODO: flip = np.random.choicec([True, False])\n",
        "\n",
        "# #test example\n",
        "# batch_index = 2 #start from 0\n",
        "# batch_size = 2\n",
        "# id = np.arange(len(valid[\"image\"]))\n",
        "# idx = id[batch_index*batch_size : (batch_index+1)*batch_size]\n",
        "# batch = [id[i] for i in idx]\n",
        "# image = np.empty((batch_size, *(1024,1280), 3))\n",
        "# depth = np.empty((batch_size, *(1024,1280), 3))\n",
        "# for i, j in enumerate(batch):  #batch:[4, 5] list #idx:[4 5] numpy array\n",
        "#   image[i] = cv2.imread(path+valid[\"image\"][j])\n",
        "#   depth[i] = tifffile.imread(path+valid[\"depth\"][j]).astype(np.float32).squeeze()\n",
        "# print(image.shape, depth.shape)\n",
        "# plt.imshow(image[0])"
      ],
      "metadata": {
        "id": "Apm6OWIYMN5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}