{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbXsEWJ8laEkA51mUuBAGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/SAM_ASF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download"
      ],
      "metadata": {
        "id": "frjLnqLGq7uO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZTjtG4Pkqwb",
        "outputId": "90c92534-1ced-43bb-e6df-170928ce6f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !rm -rf /content/thesis\n",
        "# !git clone https://github.com/ItsShi/thesis.git\n",
        "!pip -q install timm tensorboardX==1.4 torchvision==0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1LrtAZ5NITeLYkS7Bsn-r7_OEzz8awizo/view?usp=sharing\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1LrtAZ5NITeLYkS7Bsn-r7_OEzz8awizo'\n",
        "gdown.download(url,'weights_10.zip',quiet=True)\n",
        "!unzip -q weights_10.zip"
      ],
      "metadata": {
        "id": "ydv_KlzcjRSA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1zGXr3fTI-zdm_A9gViQrzzJ2qFvbhGN-/view?usp=sharing\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1zGXr3fTI-zdm_A9gViQrzzJ2qFvbhGN-'\n",
        "gdown.download(url,'weights_12.zip',quiet=True)\n",
        "!unzip -q weights_12.zip"
      ],
      "metadata": {
        "id": "MB3OFoOrpGkf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "perfect_gt_url = 'https://drive.google.com/uc?id=1GHX4ML7kUP4xFZSi56VtpZSxZdwHa2oO'\n",
        "gdown.download(perfect_gt_url,'perfect_gt.zip',quiet=True)\n",
        "!unzip -q perfect_gt.zip\n",
        "\n",
        "import gdown\n",
        "\n",
        "!mkdir videos\n",
        "d3k1_url = 'https://drive.google.com/uc?id=17k1-CHptG_XTXVX8qp7yK_zadH54vVnY'\n",
        "gdown.download(d3k1_url,'videos/d3k1_rgb.mp4',quiet=True)\n",
        "\n",
        "!pip install ffmpeg\n",
        "\n",
        "video_name = \"d3k1\"\n",
        "!ffmpeg -i /content/videos/{video_name}_rgb.mp4 -filter:v \"crop=1280:1024:0:0\" /content/videos/{video_name}_crop_rgb.mp4\n",
        "!ffmpeg -i /content/videos/{video_name}_crop_rgb.mp4 %6d.jpg\n",
        "!mkdir -p /content/frames/{video_name}\n",
        "!mv *.jpg /content/frames/{video_name}\n",
        "\n",
        "import PIL.Image as pil\n",
        "im = pil.open(\"/content/frames/{}/000001.jpg\".format(video_name))\n",
        "print(im.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kff36gnHpSb6",
        "outputId": "02c186dc-55bd-4c53-ef06-fec7f359ad37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=5b796eeb67defb1b0384c19befcf76ba076d3deefca3d3f511a2754eaee76a91\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/videos/d3k1_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf56.40.101\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 31151 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x2048, 31148 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mprofile High 4:4:4 Predictive, level 3.2, 4:4:4, 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0m264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/videos/d3k1_crop_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv444p(progressive), 1280x1024, q=2-31, 25 fps, 12800 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
            "frame=  329 fps=5.0 q=-1.0 Lsize=    7267kB time=00:00:13.04 bitrate=4565.0kbits/s speed=0.199x    \n",
            "video:7262kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.065881%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mframe I:3     Avg QP:22.27  size:144822\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mframe P:83    Avg QP:24.16  size: 53923\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mframe B:243   Avg QP:27.92  size: 10392\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mconsecutive B-frames:  1.2%  0.6%  0.9% 97.3%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mmb I  I16..4:  0.3% 80.3% 19.4%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mmb P  I16..4:  0.2%  5.3%  0.3%  P16..4: 45.3% 32.1% 13.7%  0.0%  0.0%    skip: 3.1%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mmb B  I16..4:  0.0%  0.2%  0.0%  B16..8: 53.9%  5.6%  0.9%  direct: 2.6%  skip:36.9%  L0:42.8% L1:50.1% BI: 7.1%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0m8x8 transform intra:87.9% inter:73.8%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mcoded y,u,v intra: 89.4% 25.9% 38.7% inter: 29.9% 2.5% 4.6%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mi16 v,h,dc,p: 10% 21% 17% 52%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 10% 13% 16%  6% 14% 10% 14%  7% 10%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 13% 13% 10%  7% 17% 12% 12%  8%  8%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mWeighted P-Frames: Y:4.8% UV:2.4%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mref P L0: 54.3% 18.7% 23.6%  3.3%  0.1%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mref B L0: 94.8%  4.7%  0.5%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mref B L1: 98.8%  1.2%\n",
            "\u001b[1;36m[libx264 @ 0x5b5eeb49ad80] \u001b[0mkb/s:4520.00\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/videos/d3k1_crop_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 4523 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x1024, 4520 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x5686d6ffe0c0] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '%6d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: mjpeg, yuvj444p(pc, progressive), 1280x1024, q=2-31, 200 kb/s, 25 fps, 25 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=  329 fps= 28 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=1.11x    \n",
            "video:19491kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
            "(1280, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "LyK3WQGYq_9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change image path in datasets/scared_dataset.py\n",
        "def replace_text_in_file(search_path, replace_path, search_text, replace_text):\n",
        "    with open(search_path, 'r') as file: content = file.read()\n",
        "    for i in range(len(search_text)):\n",
        "      modified_content = content.replace(search_text[i], replace_text[i])\n",
        "      content = modified_content\n",
        "    with open(replace_path, 'w') as file: file.write(modified_content)\n",
        "\n",
        "# change splits/endovis/_files.txt\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/thesis/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if int(frame)<20 and int(frame)>0:\n",
        "        if folder == \"dataset3/keyframe1\" or folder == \"dataset3/keyframe3\" or folder == \"dataset3/keyframe4\":\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "    !cp -f /content/{opt}_files.txt /content/thesis/splits/endovis/{opt}_files.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNrAZKG91SNT",
        "outputId": "f47237f0-c1c6-4070-ae16-f93acf1ebbaf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "16\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/train_end_to_end.py --no_cuda --scales 0 1 2 --data_path frames/d3k1 --log_dir sam  --batch_size 3 #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fchDZZIHp6nX",
        "outputId": "550a2e3b-10ae-4755-f808-2c57ac056044"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 48  80 128] [0, 1, 2]\n",
            "Training model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   sam\n",
            "Training is using:\n",
            "   cpu\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 16 training items and 16 validation items\n",
            "\n",
            "Training\n",
            "/content/thesis/networks/depth_encoder.py:35: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.hidden_dim)\n",
            "epoch:0/20, loss:0.095608, best loss:0.095608, best epoch:0\n",
            "Training\n",
            "epoch:1/20, loss:0.108061, best loss:0.095608, best epoch:0\n",
            "Training\n",
            "epoch:2/20, loss:0.080329, best loss:0.080329, best epoch:2\n",
            "Training\n",
            "epoch:3/20, loss:0.065610, best loss:0.065610, best epoch:3\n",
            "Training\n",
            "epoch:4/20, loss:0.107685, best loss:0.065610, best epoch:3\n",
            "Training\n",
            "epoch:5/20, loss:0.086200, best loss:0.065610, best epoch:3\n",
            "Training\n",
            "epoch:6/20, loss:0.108127, best loss:0.065610, best epoch:3\n",
            "Training\n",
            "epoch:7/20, loss:0.074380, best loss:0.065610, best epoch:3\n",
            "Training\n",
            "epoch:8/20, loss:0.065440, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:9/20, loss:0.103028, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:10/20, loss:0.082238, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:11/20, loss:0.109965, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:12/20, loss:0.073662, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:13/20, loss:0.065644, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:14/20, loss:0.089779, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:15/20, loss:0.079260, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:16/20, loss:0.107920, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:17/20, loss:0.072732, best loss:0.065440, best epoch:8\n",
            "Training\n",
            "epoch:18/20, loss:0.064015, best loss:0.064015, best epoch:18\n",
            "Training\n",
            "epoch:19/20, loss:0.091469, best loss:0.064015, best epoch:18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate"
      ],
      "metadata": {
        "id": "LxztkviW-HAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _"
      ],
      "metadata": {
        "id": "_8d2oXl9AFLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_depth.py --eval_mono --data_path frames/d3k1 --eval_split endovis --load_weights_folder sam/mdp/models/weights_18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAKYSwwo_Qc2",
        "outputId": "a8e1430c-5b16-4535-dfbf-13a0cd61f08d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Loading weights from sam/mdp/models/weights_18\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "total samples:  2\n",
            "batch size:  12\n",
            "number of batch / epoch:  1\n",
            "-> Computing predictions with size 320x256\n",
            "/content/thesis/networks/depth_encoder.py:35: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.hidden_dim)\n",
            "/content/thesis/splits/endovis/gt_depths.npz\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/thesis/evaluate_depth.py\", line 234, in <module>\n",
            "    evaluate(options.parse())\n",
            "  File \"/content/thesis/evaluate_depth.py\", line 170, in evaluate\n",
            "    gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1')[\"data\"]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 420, in load\n",
            "    ret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 187, in __init__\n",
            "    _zip = zipfile_factory(fid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n",
            "    return zipfile.ZipFile(file, *args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 1269, in __init__\n",
            "    self._RealGetContents()\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 1336, in _RealGetContents\n",
            "    raise BadZipFile(\"File is not a zip file\")\n",
            "zipfile.BadZipFile: File is not a zip file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_pose.py --data_path SCARED --eval_split endovis --load_weights_folder sam/mdp/models/weights_18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suz6lgXT9Wfj",
        "outputId": "e753bb6c-5c01-42e0-b8f8-ab9154915915"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "total samples:  833\n",
            "batch size:  12\n",
            "number of batch / epoch:  70\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/thesis/evaluate_pose.py\", line 145, in <module>\n",
            "    evaluate(options.parse())\n",
            "  File \"/content/thesis/evaluate_pose.py\", line 97, in evaluate\n",
            "    pose_encoder.cuda()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 688, in cuda\n",
            "    return self._apply(lambda t: t.cuda(device))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 578, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 578, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 601, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 688, in <lambda>\n",
            "    return self._apply(lambda t: t.cuda(device))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 216, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/test_simple.py --model_path sam/mdp/models/weights_18 --image_path SCARED --no_cuda --ext png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqdK49T-opOT",
        "outputId": "62a4939e-7d77-4042-b30d-cdcb2db944b8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Loading model from  sam/mdp/models/weights_18\n",
            "   Loading pretrained encoder\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/thesis/test_simple.py\", line 138, in <module>\n",
            "    test_simple(args)\n",
            "  File \"/content/thesis/test_simple.py\", line 60, in test_simple\n",
            "    encoder.load_state_dict(filtered_dict_enc)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1497, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for ResnetEncoder:\n",
            "\tMissing key(s) in state_dict: \"encoder.conv1.weight\", \"encoder.bn1.weight\", \"encoder.bn1.bias\", \"encoder.bn1.running_mean\", \"encoder.bn1.running_var\", \"encoder.layer1.0.conv1.weight\", \"encoder.layer1.0.bn1.weight\", \"encoder.layer1.0.bn1.bias\", \"encoder.layer1.0.bn1.running_mean\", \"encoder.layer1.0.bn1.running_var\", \"encoder.layer1.0.conv2.weight\", \"encoder.layer1.0.bn2.weight\", \"encoder.layer1.0.bn2.bias\", \"encoder.layer1.0.bn2.running_mean\", \"encoder.layer1.0.bn2.running_var\", \"encoder.layer1.1.conv1.weight\", \"encoder.layer1.1.bn1.weight\", \"encoder.layer1.1.bn1.bias\", \"encoder.layer1.1.bn1.running_mean\", \"encoder.layer1.1.bn1.running_var\", \"encoder.layer1.1.conv2.weight\", \"encoder.layer1.1.bn2.weight\", \"encoder.layer1.1.bn2.bias\", \"encoder.layer1.1.bn2.running_mean\", \"encoder.layer1.1.bn2.running_var\", \"encoder.layer2.0.conv1.weight\", \"encoder.layer2.0.bn1.weight\", \"encoder.layer2.0.bn1.bias\", \"encoder.layer2.0.bn1.running_mean\", \"encoder.layer2.0.bn1.running_var\", \"encoder.layer2.0.conv2.weight\", \"encoder.layer2.0.bn2.weight\", \"encoder.layer2.0.bn2.bias\", \"encoder.layer2.0.bn2.running_mean\", \"encoder.layer2.0.bn2.running_var\", \"encoder.layer2.0.downsample.0.weight\", \"encoder.layer2.0.downsample.1.weight\", \"encoder.layer2.0.downsample.1.bias\", \"encoder.layer2.0.downsample.1.running_mean\", \"encoder.layer2.0.downsample.1.running_var\", \"encoder.layer2.1.conv1.weight\", \"encoder.layer2.1.bn1.weight\", \"encoder.layer2.1.bn1.bias\", \"encoder.layer2.1.bn1.running_mean\", \"encoder.layer2.1.bn1.running_var\", \"encoder.layer2.1.conv2.weight\", \"encoder.layer2.1.bn2.weight\", \"encoder.layer2.1.bn2.bias\", \"encoder.layer2.1.bn2.running_mean\", \"encoder.layer2.1.bn2.running_var\", \"encoder.layer3.0.conv1.weight\", \"encoder.layer3.0.bn1.weight\", \"encoder.layer3.0.bn1.bias\", \"encoder.layer3.0.bn1.running_mean\", \"encoder.layer3.0.bn1.running_var\", \"encoder.layer3.0.conv2.weight\", \"encoder.layer3.0.bn2.weight\", \"encoder.layer3.0.bn2.bias\", \"encoder.layer3.0.bn2.running_mean\", \"encoder.layer3.0.bn2.running_var\", \"encoder.layer3.0.downsample.0.weight\", \"encoder.layer3.0.downsample.1.weight\", \"encoder.layer3.0.downsample.1.bias\", \"encoder.layer3.0.downsample.1.running_mean\", \"encoder.layer3.0.downsample.1.running_var\", \"encoder.layer3.1.conv1.weight\", \"encoder.layer3.1.bn1.weight\", \"encoder.layer3.1.bn1.bias\", \"encoder.layer3.1.bn1.running_mean\", \"encoder.layer3.1.bn1.running_var\", \"encoder.layer3.1.conv2.weight\", \"encoder.layer3.1.bn2.weight\", \"encoder.layer3.1.bn2.bias\", \"encoder.layer3.1.bn2.running_mean\", \"encoder.layer3.1.bn2.running_var\", \"encoder.layer4.0.conv1.weight\", \"encoder.layer4.0.bn1.weight\", \"encoder.layer4.0.bn1.bias\", \"encoder.layer4.0.bn1.running_mean\", \"encoder.layer4.0.bn1.running_var\", \"encoder.layer4.0.conv2.weight\", \"encoder.layer4.0.bn2.weight\", \"encoder.layer4.0.bn2.bias\", \"encoder.layer4.0.bn2.running_mean\", \"encoder.layer4.0.bn2.running_var\", \"encoder.layer4.0.downsample.0.weight\", \"encoder.layer4.0.downsample.1.weight\", \"encoder.layer4.0.downsample.1.bias\", \"encoder.layer4.0.downsample.1.running_mean\", \"encoder.layer4.0.downsample.1.running_var\", \"encoder.layer4.1.conv1.weight\", \"encoder.layer4.1.bn1.weight\", \"encoder.layer4.1.bn1.bias\", \"encoder.layer4.1.bn1.running_mean\", \"encoder.layer4.1.bn1.running_var\", \"encoder.layer4.1.conv2.weight\", \"encoder.layer4.1.bn2.weight\", \"encoder.layer4.1.bn2.bias\", \"encoder.layer4.1.bn2.running_mean\", \"encoder.layer4.1.bn2.running_var\", \"encoder.fc.weight\", \"encoder.fc.bias\". \n"
          ]
        }
      ]
    }
  ]
}