{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOInThuMx2Z0pDA+8vpM2Np",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/ASF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download"
      ],
      "metadata": {
        "id": "frjLnqLGq7uO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZTjtG4Pkqwb",
        "outputId": "ce56729a-9f4e-4912-8b64-388c31065ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asdf'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 45 (delta 11), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (45/45), 282.78 KiB | 4.28 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/asdf\n",
        "!git clone https://github.com/ItsShi/asdf.git\n",
        "\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/asdf/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if int(frame)<20 and int(frame)>0:\n",
        "        if folder == \"dataset3/keyframe1\" or folder == \"dataset3/keyframe3\" or folder == \"dataset3/keyframe4\":\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "    !cp -f /content/{opt}_files.txt /content/asdf/splits/endovis/{opt}_files.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm tensorboardX==1.4 torchvision==0.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aMAWtFhq1cz",
        "outputId": "a45a273d-e456-4a48-e056-3d523f9b1478"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m917.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "perfect_gt_url = 'https://drive.google.com/uc?id=1GHX4ML7kUP4xFZSi56VtpZSxZdwHa2oO'\n",
        "gdown.download(perfect_gt_url,'perfect_gt.zip',quiet=True)\n",
        "!unzip -q perfect_gt.zip\n",
        "\n",
        "import gdown\n",
        "\n",
        "!mkdir videos\n",
        "d3k1_url = 'https://drive.google.com/uc?id=17k1-CHptG_XTXVX8qp7yK_zadH54vVnY'\n",
        "gdown.download(d3k1_url,'videos/d3k1_rgb.mp4',quiet=True)\n",
        "# frame=  329 fps= 23 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=0.904x\n",
        "\n",
        "!pip install ffmpeg\n",
        "\n",
        "video_name = \"d3k1\"\n",
        "!ffmpeg -i /content/videos/{video_name}_rgb.mp4 -filter:v \"crop=1280:1024:0:0\" /content/videos/{video_name}_crop_rgb.mp4\n",
        "!ffmpeg -i /content/videos/{video_name}_crop_rgb.mp4 %6d.jpg\n",
        "!mkdir -p /content/frames/{video_name}\n",
        "!mv *.jpg /content/frames/{video_name}\n",
        "\n",
        "import PIL.Image as pil\n",
        "im = pil.open(\"/content/frames/{}/000001.jpg\".format(video_name))\n",
        "print(im.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kff36gnHpSb6",
        "outputId": "b3be5d3a-67ac-49e7-c2f3-16c3fcaf84db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=620a8267cc6c96280301ade0bc0f1913f3e691847266814a9159d4376f50b9d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/videos/d3k1_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf56.40.101\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 31151 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x2048, 31148 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mprofile High 4:4:4 Predictive, level 3.2, 4:4:4, 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0m264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/videos/d3k1_crop_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv444p(progressive), 1280x1024, q=2-31, 25 fps, 12800 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
            "frame=  329 fps=4.8 q=-1.0 Lsize=    7267kB time=00:00:13.04 bitrate=4565.0kbits/s speed=0.189x    \n",
            "video:7262kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.065881%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mframe I:3     Avg QP:22.27  size:144822\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mframe P:83    Avg QP:24.16  size: 53923\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mframe B:243   Avg QP:27.92  size: 10392\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mconsecutive B-frames:  1.2%  0.6%  0.9% 97.3%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mmb I  I16..4:  0.3% 80.3% 19.4%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mmb P  I16..4:  0.2%  5.3%  0.3%  P16..4: 45.3% 32.1% 13.7%  0.0%  0.0%    skip: 3.1%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mmb B  I16..4:  0.0%  0.2%  0.0%  B16..8: 53.9%  5.6%  0.9%  direct: 2.6%  skip:36.9%  L0:42.8% L1:50.1% BI: 7.1%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0m8x8 transform intra:87.9% inter:73.8%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mcoded y,u,v intra: 89.4% 25.9% 38.7% inter: 29.9% 2.5% 4.6%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mi16 v,h,dc,p: 10% 21% 17% 52%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 10% 13% 16%  6% 14% 10% 14%  7% 10%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 13% 13% 10%  7% 17% 12% 12%  8%  8%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mWeighted P-Frames: Y:4.8% UV:2.4%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mref P L0: 54.3% 18.7% 23.6%  3.3%  0.1%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mref B L0: 94.8%  4.7%  0.5%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mref B L1: 98.8%  1.2%\n",
            "\u001b[1;36m[libx264 @ 0x5a86e0835d80] \u001b[0mkb/s:4520.00\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/videos/d3k1_crop_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 4523 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x1024, 4520 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x5646623d30c0] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '%6d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: mjpeg, yuvj444p(pc, progressive), 1280x1024, q=2-31, 200 kb/s, 25 fps, 25 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=  329 fps= 27 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=1.08x    \n",
            "video:19491kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
            "(1280, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change image path in datasets/scared_dataset.py\n",
        "def replace_text_in_file(search_path, replace_path, search_text, replace_text):\n",
        "    with open(search_path, 'r') as file: content = file.read()\n",
        "    for i in range(len(search_text)):\n",
        "      modified_content = content.replace(search_text[i], replace_text[i])\n",
        "      content = modified_content\n",
        "    with open(replace_path, 'w') as file: file.write(modified_content)\n",
        "\n",
        "# change splits/endovis/_files.txt\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/asdf/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if int(frame)<20 and int(frame)>0:\n",
        "        if folder == \"dataset3/keyframe1\" or folder == \"dataset3/keyframe3\" or folder == \"dataset3/keyframe4\":\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "    !cp -f /content/{opt}_files.txt /content/asdf/splits/endovis/{opt}_files.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNrAZKG91SNT",
        "outputId": "54183b16-069b-4e1e-ed67-c2e369b91e2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "16\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python asdf/train_end_to_end.py --no_cuda --data_path frames/d3k1 --log_dir asdf  --num_epochs 1 --batch_size 1 # --scales 0 1 2  #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-iuEgjx6yw",
        "outputId": "2568de7a-37f4-48a4-dd63-ba5cb7d746c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   asdf\n",
            "Training is using:\n",
            "   cpu\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 16 training items and 16 validation items\n",
            "\n",
            "Training..\n",
            "Processing batch..\n",
            "input:  torch.Size([1, 3, 256, 320])\n",
            "features:  5 torch.Size([1, 64, 128, 160]) torch.Size([1, 64, 64, 80]) torch.Size([1, 128, 32, 40]) torch.Size([1, 256, 16, 20]) torch.Size([1, 512, 8, 10])\n",
            "output:  torch.Size([1, 1, 256, 320])\n",
            "tensor(0.0630, grad_fn=<DivBackward0>)\n",
            "Processing batch..\n",
            "input:  torch.Size([1, 3, 256, 320])\n",
            "features:  5 torch.Size([1, 64, 128, 160]) torch.Size([1, 64, 64, 80]) torch.Size([1, 128, 32, 40]) torch.Size([1, 256, 16, 20]) torch.Size([1, 512, 8, 10])\n",
            "output:  torch.Size([1, 1, 256, 320])\n",
            "epoch:0/1, loss:0.065742, best loss:0.065742, best epoch:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1zGXr3fTI-zdm_A9gViQrzzJ2qFvbhGN-'\n",
        "gdown.download(url,'weights_12.zip',quiet=True)\n",
        "!unzip -q weights_12.zip"
      ],
      "metadata": {
        "id": "9NAsbTVcgdi7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test sequence1: d5k4 dataset5/keyframe4\t1\tl\n",
        "# test sequence2 d3k4\n",
        "# seq3 d3k1 frames/d3k1/000001.jpg\n",
        "\n",
        "# import gdown\n",
        "# import shutil\n",
        "\n",
        "# video_name = \"d3k1\"\n",
        "# camera_url = 'https://drive.google.com/uc?id=1_G5h2CUXqdREPj8dB7DorVDQHxP4oLvF'\n",
        "# gdown.download(camera_url,'SCARED_camera.zip',quiet=True)\n",
        "# !unzip -q /content/SCARED_camera.zip\n",
        "# shutil.unpack_archive(\"SCARED_camera/{}_frame_data.tar.gz\".format(video_name), \"{}_json\".format(video_name))\n",
        "\n",
        "# !python thesis/export_gt_pose.py --data_path d3k1_json --split endovis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuqWKixFtYKP",
        "outputId": "7f04c98d-ba33-48b1-e1f7-ec542a64d39c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to endovis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"{}_sequence.txt\".format(video_name), \"w+\") as f:\n",
        "    for i in range(2, len(os.listdir('d3k1_json')) + 1):\n",
        "        f.write(\"dataset{}/keyframe{}\\t{}\\tl\\n\".format(video_name[1], video_name[-1],i))\n",
        "\n",
        "!cp d3k1_sequence.txt thesis/splits/endovis/"
      ],
      "metadata": {
        "id": "g5Wzpd5S10dn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_pose.py --data_path frames/d3k1 --eval_split endovis --load_weights_folder weights_12"
      ],
      "metadata": {
        "id": "suz6lgXT9Wfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_pose.py --data_path frames/d3k1 --eval_split endovis --load_weights_folder weights_12\n"
      ],
      "metadata": {
        "id": "NN4LeHa66eTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAM"
      ],
      "metadata": {
        "id": "UYsK3WLye6zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate"
      ],
      "metadata": {
        "id": "LxztkviW-HAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lite-mono only use 3 scales"
      ],
      "metadata": {
        "id": "LyK3WQGYq_9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/train_end_to_end.py --no_cuda --scales 0 1 2 --data_path frames/d3k1 --log_dir sam  --batch_size 3 #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\""
      ],
      "metadata": {
        "id": "fchDZZIHp6nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvOs114We8zd",
        "outputId": "1371c5ef-24eb-4364-f5ac-02c9d6769796"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-09-07 16:32:03--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.38, 13.35.7.128, 13.35.7.82, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_b_01ec64.pth’\n",
            "\n",
            "sam_vit_b_01ec64.pt 100%[===================>] 357.67M  39.8MB/s    in 5.6s    \n",
            "\n",
            "2023-09-07 16:32:08 (64.1 MB/s) - ‘sam_vit_b_01ec64.pth’ saved [375042383/375042383]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "sam_checkpoint = \"/content/sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "# device = \"cuda\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
        "sam.eval()\n",
        "#if you want to change the size:\n",
        "# sam, img_embedding_size = sam_model_registry[model_type](image_size=img_size,\n",
        "#                                     num_classes=num_classes,\n",
        "#                                     checkpoint=sam_checkpoint, pixel_mean=[0, 0, 0],\n",
        "#                                     pixel_std=[1, 1, 1])\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# raw_img = Image.open('/content/raw_img.jpg')\n",
        "# image = np.array(raw_img)\n",
        "# masks = mask_generator.generate(image)\n",
        "# print(masks[0].keys())"
      ],
      "metadata": {
        "id": "ZVpvk1TvfJDj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woSwzF96fbtD",
        "outputId": "7fba359e-5205-426a-91eb-42384f065c76"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<segment_anything.automatic_mask_generator.SamAutomaticMaskGenerator object at 0x7f1a33f9aa40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_encoder = sam.image_encoder"
      ],
      "metadata": {
        "id": "HM4EK3vRfhRj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input: torch.Size([3, 3, 256, 320])\n",
        "# features: torch.Size([3, 48, 64, 80]) torch.Size([3, 80, 32, 40]) torch.Size([3, 128, 16, 20])\n",
        "# output: torch.Size([3, 1, 256, 320])"
      ],
      "metadata": {
        "id": "hTsXr1eXinSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/train_end_to_end.py --no_cuda --height 224 --width 224 --scales 0 1 2 --data_path frames/d3k1 --log_dir sam  --batch_size 3 #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o94vSJQzgguP",
        "outputId": "28964db5-eb8d-4c82-b9a3-6d5e32eb35b6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "[ 48  80 128] [0]\n",
            "Training model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   sam\n",
            "Training is using:\n",
            "   cpu\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 16 training items and 16 validation items\n",
            "\n",
            "Training\n",
            "begining swin x: torch.Size([3, 48, 56, 56])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/thesis/train_end_to_end.py\", line 12, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 209, in train\n",
            "    val_losses = self.run_epoch()\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 228, in run_epoch\n",
            "    outputs, losses = self.process_batch(inputs)\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 259, in process_batch\n",
            "    features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 465, in forward\n",
            "    x = self.forward_features(x)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 445, in forward_features\n",
            "    x = self.stages[0][-1](x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 296, in forward\n",
            "    x = self.swintran.patch_embed(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/patch_embed.py\", line 72, in forward\n",
            "    _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 772, in _assert\n",
            "    assert condition, message\n",
            "AssertionError: Input height (56) doesn't match model (224).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [\"encoder = networks.ResnetEncoder(18, False)\"]\n",
        "# [\"encoder = networks.LiteMono(model=\"lite-mono\", width=feed_width, height=feed_height)\"]\n",
        "\n",
        "# depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
        "\n",
        "# scales=range(4)"
      ],
      "metadata": {
        "id": "DF981Fy9P31H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/test_simple.py --model_path sam/mdp/models/weights_18 --image_path SCARED --no_cuda --ext png"
      ],
      "metadata": {
        "id": "lqdK49T-opOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/export_gt_depth.py"
      ],
      "metadata": {
        "id": "yAP6hYGbO2Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_depth.py --eval_mono --data_path frames/d3k1 --eval_split endovis --load_weights_folder sam/mdp/models/weights_18\n",
        "\n",
        "# import numpy as np\n",
        "# gt_depths = np.load(\"thesis/splits/endovis/gt_depths.npz\")\n",
        "# gt_depths = np.load(\"thesis/splits/endovis/gt_depths.npz\", fix_imports=True, encoding='latin1')[\"data\"]"
      ],
      "metadata": {
        "id": "IAKYSwwo_Qc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LGFI(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Local-Global Features Interaction\n",
        "#     \"\"\"\n",
        "#     def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=6,\n",
        "#                  use_pos_emb=True, num_heads=6, qkv_bias=True, attn_drop=0., drop=0.):\n",
        "#         super().__init__()\n",
        "#         self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "#         # self.swintran.eval()\n",
        "#         # for param in self.swintran.parameters():\n",
        "#         #     param.requires_grad = False\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       print(\"begining swin x:\", x.size())\n",
        "\n",
        "#       x = self.swintran.patch_embed(x)\n",
        "#       x = self.swintran.layers(x)\n",
        "#       x = self.swintran.norm(x).permute(0,3,1,2)\n",
        "#       print(\"end swin x:\", x.size())\n",
        "\n",
        "#       return x"
      ],
      "metadata": {
        "id": "sWB2Zl8bcjxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}