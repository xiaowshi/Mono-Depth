{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8NfF3fhS+D1ww6UC5LS9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/ASF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download"
      ],
      "metadata": {
        "id": "frjLnqLGq7uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm tensorboardX==1.4 torchvision==0.12.0"
      ],
      "metadata": {
        "id": "5gr3MiQSM2mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZTjtG4Pkqwb",
        "outputId": "de614e3d-50cc-4558-ba92-cd190da814c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asdf'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/58)\u001b[K\rremote: Counting objects:   3% (2/58)\u001b[K\rremote: Counting objects:   5% (3/58)\u001b[K\rremote: Counting objects:   6% (4/58)\u001b[K\rremote: Counting objects:   8% (5/58)\u001b[K\rremote: Counting objects:  10% (6/58)\u001b[K\rremote: Counting objects:  12% (7/58)\u001b[K\rremote: Counting objects:  13% (8/58)\u001b[K\rremote: Counting objects:  15% (9/58)\u001b[K\rremote: Counting objects:  17% (10/58)\u001b[K\rremote: Counting objects:  18% (11/58)\u001b[K\rremote: Counting objects:  20% (12/58)\u001b[K\rremote: Counting objects:  22% (13/58)\u001b[K\rremote: Counting objects:  24% (14/58)\u001b[K\rremote: Counting objects:  25% (15/58)\u001b[K\rremote: Counting objects:  27% (16/58)\u001b[K\rremote: Counting objects:  29% (17/58)\u001b[K\rremote: Counting objects:  31% (18/58)\u001b[K\rremote: Counting objects:  32% (19/58)\u001b[K\rremote: Counting objects:  34% (20/58)\u001b[K\rremote: Counting objects:  36% (21/58)\u001b[K\rremote: Counting objects:  37% (22/58)\u001b[K\rremote: Counting objects:  39% (23/58)\u001b[K\rremote: Counting objects:  41% (24/58)\u001b[K\rremote: Counting objects:  43% (25/58)\u001b[K\rremote: Counting objects:  44% (26/58)\u001b[K\rremote: Counting objects:  46% (27/58)\u001b[K\rremote: Counting objects:  48% (28/58)\u001b[K\rremote: Counting objects:  50% (29/58)\u001b[K\rremote: Counting objects:  51% (30/58)\u001b[K\rremote: Counting objects:  53% (31/58)\u001b[K\rremote: Counting objects:  55% (32/58)\u001b[K\rremote: Counting objects:  56% (33/58)\u001b[K\rremote: Counting objects:  58% (34/58)\u001b[K\rremote: Counting objects:  60% (35/58)\u001b[K\rremote: Counting objects:  62% (36/58)\u001b[K\rremote: Counting objects:  63% (37/58)\u001b[K\rremote: Counting objects:  65% (38/58)\u001b[K\rremote: Counting objects:  67% (39/58)\u001b[K\rremote: Counting objects:  68% (40/58)\u001b[K\rremote: Counting objects:  70% (41/58)\u001b[K\rremote: Counting objects:  72% (42/58)\u001b[K\rremote: Counting objects:  74% (43/58)\u001b[K\rremote: Counting objects:  75% (44/58)\u001b[K\rremote: Counting objects:  77% (45/58)\u001b[K\rremote: Counting objects:  79% (46/58)\u001b[K\rremote: Counting objects:  81% (47/58)\u001b[K\rremote: Counting objects:  82% (48/58)\u001b[K\rremote: Counting objects:  84% (49/58)\u001b[K\rremote: Counting objects:  86% (50/58)\u001b[K\rremote: Counting objects:  87% (51/58)\u001b[K\rremote: Counting objects:  89% (52/58)\u001b[K\rremote: Counting objects:  91% (53/58)\u001b[K\rremote: Counting objects:  93% (54/58)\u001b[K\rremote: Counting objects:  94% (55/58)\u001b[K\rremote: Counting objects:  96% (56/58)\u001b[K\rremote: Counting objects:  98% (57/58)\u001b[K\rremote: Counting objects: 100% (58/58)\u001b[K\rremote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 58 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (58/58), 289.12 KiB | 4.59 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "16\n",
            "16\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/asdf\n",
        "!git clone https://github.com/ItsShi/asdf.git\n",
        "\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/asdf/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if int(frame)<20 and int(frame)>0:\n",
        "        if folder == \"dataset3/keyframe1\" or folder == \"dataset3/keyframe3\" or folder == \"dataset3/keyframe4\":\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "    !cp -f /content/{opt}_files.txt /content/asdf/splits/endovis/{opt}_files.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gdown\n",
        "\n",
        "# perfect_gt_url = 'https://drive.google.com/uc?id=1GHX4ML7kUP4xFZSi56VtpZSxZdwHa2oO'\n",
        "# gdown.download(perfect_gt_url,'perfect_gt.zip',quiet=True)\n",
        "# !unzip -q perfect_gt.zip\n",
        "\n",
        "import gdown\n",
        "\n",
        "!mkdir videos\n",
        "d3k1_url = 'https://drive.google.com/uc?id=17k1-CHptG_XTXVX8qp7yK_zadH54vVnY'\n",
        "gdown.download(d3k1_url,'videos/d3k1_rgb.mp4',quiet=True)\n",
        "# frame=  329 fps= 23 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=0.904x\n",
        "\n",
        "!pip install ffmpeg\n",
        "\n",
        "video_name = \"d3k1\"\n",
        "!ffmpeg -i /content/videos/{video_name}_rgb.mp4 -filter:v \"crop=1280:1024:0:0\" /content/videos/{video_name}_crop_rgb.mp4\n",
        "!ffmpeg -i /content/videos/{video_name}_crop_rgb.mp4 %6d.jpg\n",
        "!mkdir -p /content/frames/{video_name}\n",
        "!mv *.jpg /content/frames/{video_name}\n",
        "\n",
        "import PIL.Image as pil\n",
        "im = pil.open(\"/content/frames/{}/000001.jpg\".format(video_name))\n",
        "print(im.size)"
      ],
      "metadata": {
        "id": "Kff36gnHpSb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python asdf/train_end_to_end.py --no_cuda --data_path frames/d3k1 --log_dir asdf  --num_epochs 1 --batch_size 1 --width 224 --height 224 --scales 0 # --scales 0 1 2  #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-iuEgjx6yw",
        "outputId": "d933ff49-d54f-4e58-f25a-6b6afdc4a5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   asdf\n",
            "Training is using:\n",
            "   cpu\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Using split:\n",
            "   endovis\n",
            "There are 16 training items and 16 validation items\n",
            "\n",
            "Training\n",
            "epoch   0 | batch      0 | examples/s:   0.1 | loss: 0.10914 | time elapsed: 00h00m14s | time left: 00h00m00s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"{}_sequence.txt\".format(video_name), \"w+\") as f:\n",
        "    for i in range(2, len(os.listdir('d3k1_json')) + 1):\n",
        "        f.write(\"dataset{}/keyframe{}\\t{}\\tl\\n\".format(video_name[1], video_name[-1],i))\n",
        "\n",
        "!cp d3k1_sequence.txt thesis/splits/endovis/"
      ],
      "metadata": {
        "id": "g5Wzpd5S10dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_pose.py --data_path frames/d3k1 --eval_split endovis --load_weights_folder weights_12"
      ],
      "metadata": {
        "id": "suz6lgXT9Wfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test sequence1: d5k4 dataset5/keyframe4\t1\tl\n",
        "# test sequence2 d3k4\n",
        "# seq3 d3k1 frames/d3k1/000001.jpg\n",
        "\n",
        "# import gdown\n",
        "# import shutil\n",
        "\n",
        "# video_name = \"d3k1\"\n",
        "# camera_url = 'https://drive.google.com/uc?id=1_G5h2CUXqdREPj8dB7DorVDQHxP4oLvF'\n",
        "# gdown.download(camera_url,'SCARED_camera.zip',quiet=True)\n",
        "# !unzip -q /content/SCARED_camera.zip\n",
        "# shutil.unpack_archive(\"SCARED_camera/{}_frame_data.tar.gz\".format(video_name), \"{}_json\".format(video_name))\n",
        "\n",
        "# !python thesis/export_gt_pose.py --data_path d3k1_json --split endovis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuqWKixFtYKP",
        "outputId": "7f04c98d-ba33-48b1-e1f7-ec542a64d39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to endovis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_pose.py --data_path frames/d3k1 --eval_split endovis --load_weights_folder weights_12\n"
      ],
      "metadata": {
        "id": "NN4LeHa66eTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAM"
      ],
      "metadata": {
        "id": "UYsK3WLye6zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate"
      ],
      "metadata": {
        "id": "LxztkviW-HAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lite-mono only use 3 scales"
      ],
      "metadata": {
        "id": "LyK3WQGYq_9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/train_end_to_end.py --no_cuda --scales 0 1 2 --data_path frames/d3k1 --log_dir sam  --batch_size 3 #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\""
      ],
      "metadata": {
        "id": "fchDZZIHp6nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvOs114We8zd",
        "outputId": "1371c5ef-24eb-4364-f5ac-02c9d6769796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-09-07 16:32:03--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.38, 13.35.7.128, 13.35.7.82, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_b_01ec64.pth’\n",
            "\n",
            "sam_vit_b_01ec64.pt 100%[===================>] 357.67M  39.8MB/s    in 5.6s    \n",
            "\n",
            "2023-09-07 16:32:08 (64.1 MB/s) - ‘sam_vit_b_01ec64.pth’ saved [375042383/375042383]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "sam_checkpoint = \"/content/sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "# device = \"cuda\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
        "sam.eval()\n",
        "#if you want to change the size:\n",
        "# sam, img_embedding_size = sam_model_registry[model_type](image_size=img_size,\n",
        "#                                     num_classes=num_classes,\n",
        "#                                     checkpoint=sam_checkpoint, pixel_mean=[0, 0, 0],\n",
        "#                                     pixel_std=[1, 1, 1])\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# raw_img = Image.open('/content/raw_img.jpg')\n",
        "# image = np.array(raw_img)\n",
        "# masks = mask_generator.generate(image)\n",
        "# print(masks[0].keys())"
      ],
      "metadata": {
        "id": "ZVpvk1TvfJDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woSwzF96fbtD",
        "outputId": "7fba359e-5205-426a-91eb-42384f065c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<segment_anything.automatic_mask_generator.SamAutomaticMaskGenerator object at 0x7f1a33f9aa40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_encoder = sam.image_encoder"
      ],
      "metadata": {
        "id": "HM4EK3vRfhRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input: torch.Size([3, 3, 256, 320])\n",
        "# features: torch.Size([3, 48, 64, 80]) torch.Size([3, 80, 32, 40]) torch.Size([3, 128, 16, 20])\n",
        "# output: torch.Size([3, 1, 256, 320])"
      ],
      "metadata": {
        "id": "hTsXr1eXinSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/train_end_to_end.py --no_cuda --height 224 --width 224 --scales 0 1 2 --data_path frames/d3k1 --log_dir sam  --batch_size 3 #--load_weights_folder womin/mdp/models/weights_19  --models_to_load \"encoder\" \"depth\" \"pose_encoder\" \"pose\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o94vSJQzgguP",
        "outputId": "28964db5-eb8d-4c82-b9a3-6d5e32eb35b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "[ 48  80 128] [0]\n",
            "Training model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   sam\n",
            "Training is using:\n",
            "   cpu\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 16 training items and 16 validation items\n",
            "\n",
            "Training\n",
            "begining swin x: torch.Size([3, 48, 56, 56])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/thesis/train_end_to_end.py\", line 12, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 209, in train\n",
            "    val_losses = self.run_epoch()\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 228, in run_epoch\n",
            "    outputs, losses = self.process_batch(inputs)\n",
            "  File \"/content/thesis/trainer_end_to_end.py\", line 259, in process_batch\n",
            "    features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 465, in forward\n",
            "    x = self.forward_features(x)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 445, in forward_features\n",
            "    x = self.stages[0][-1](x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/thesis/networks/depth_encoder.py\", line 296, in forward\n",
            "    x = self.swintran.patch_embed(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/patch_embed.py\", line 72, in forward\n",
            "    _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 772, in _assert\n",
            "    assert condition, message\n",
            "AssertionError: Input height (56) doesn't match model (224).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [\"encoder = networks.ResnetEncoder(18, False)\"]\n",
        "# [\"encoder = networks.LiteMono(model=\"lite-mono\", width=feed_width, height=feed_height)\"]\n",
        "\n",
        "# depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
        "\n",
        "# scales=range(4)"
      ],
      "metadata": {
        "id": "DF981Fy9P31H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/test_simple.py --model_path sam/mdp/models/weights_18 --image_path SCARED --no_cuda --ext png"
      ],
      "metadata": {
        "id": "lqdK49T-opOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/export_gt_depth.py"
      ],
      "metadata": {
        "id": "yAP6hYGbO2Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python thesis/evaluate_depth.py --eval_mono --data_path frames/d3k1 --eval_split endovis --load_weights_folder sam/mdp/models/weights_18\n",
        "\n",
        "# import numpy as np\n",
        "# gt_depths = np.load(\"thesis/splits/endovis/gt_depths.npz\")\n",
        "# gt_depths = np.load(\"thesis/splits/endovis/gt_depths.npz\", fix_imports=True, encoding='latin1')[\"data\"]"
      ],
      "metadata": {
        "id": "IAKYSwwo_Qc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LGFI(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Local-Global Features Interaction\n",
        "#     \"\"\"\n",
        "#     def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, expan_ratio=6,\n",
        "#                  use_pos_emb=True, num_heads=6, qkv_bias=True, attn_drop=0., drop=0.):\n",
        "#         super().__init__()\n",
        "#         self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "#         # self.swintran.eval()\n",
        "#         # for param in self.swintran.parameters():\n",
        "#         #     param.requires_grad = False\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       print(\"begining swin x:\", x.size())\n",
        "\n",
        "#       x = self.swintran.patch_embed(x)\n",
        "#       x = self.swintran.layers(x)\n",
        "#       x = self.swintran.norm(x).permute(0,3,1,2)\n",
        "#       print(\"end swin x:\", x.size())\n",
        "\n",
        "#       return x"
      ],
      "metadata": {
        "id": "sWB2Zl8bcjxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}