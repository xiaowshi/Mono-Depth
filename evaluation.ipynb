{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtO3Sl83JA696gdOE58hID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGj4DakSsUmj",
        "outputId": "c6112926-1880-4dd0-9d70-1b60558d96bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MonoDepth'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 176 (delta 73), reused 83 (delta 27), pack-reused 4\u001b[K\n",
            "Receiving objects: 100% (176/176), 10.44 MiB | 19.36 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ItsShi/MonoDepth.git\n",
        "\n",
        "!pip -q install tensorboardX==1.4\n",
        "!pip -q install torchvision==0.12.0\n",
        "!pip -q install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "video_url = 'https://drive.google.com/uc?id=1c_ewx6wts7pJTb3XVWVXEZLbFXDObpdP'\n",
        "gdown.download(video_url,'SCARED_video.zip',quiet=True) \n",
        "\n",
        "!unzip -q /content/SCARED_video.zip"
      ],
      "metadata": {
        "id": "pQW2jxWsv61U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# depth gt\n",
        "!pwd\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "scene_url = 'https://drive.google.com/uc?id=1xZwB0qFWbU7EjjI1g-Y0pqAs1_nWwU6Z'\n",
        "gdown.download(scene_url, 'scene.tar.gz', quiet=True)\n",
        "\n",
        "shutil.unpack_archive('scene.tar.gz', 'd3k1_tiffs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxfRys6Bsnrb",
        "outputId": "debcb046-97f4-49a3-c794-c2223c1d104f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vid2frame\n",
        "!pip install ffmpeg\n",
        "!mkdir /content/data\n",
        "!ffmpeg -i /content/SCARED_video/d3k1_rgb.mp4 %6d.jpg  \n",
        "!mv *.jpg /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TfwO3ResZ7B",
        "outputId": "dda62c2c-f8bc-4017-84ae-c5dada220829"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg\n",
            "  Using cached ffmpeg-1.4-py3-none-any.whl\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "mkdir: cannot create directory ‘/content/data’: File exists\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/d3k1_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf56.40.101\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 31151 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x2048, 31148 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x557091ae5c40] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '%6d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0(und): Video: mjpeg, yuvj444p(pc), 1280x2048, q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      encoder         : Lavc58.54.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
            "frame=  329 fps= 15 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=0.605x    \n",
            "video:39020kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_files\n",
        "!touch train_files.txt\n",
        "!touch val_files.txt\n",
        "!touch test_files.txt\n",
        "\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/MonoDepth/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if folder == \"dataset3/keyframe4\" and int(frame)<328:\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "# move and replace the original train_files.txt file\n",
        "%pwd\n",
        "!cp /content/MonoDepth/splits/endovis/test_files.txt /content/test_files_original.txt\n",
        "!cp -f /content/val_files.txt /content/MonoDepth/splits/endovis/val_files.txt\n",
        "!cp -f /content/train_files.txt /content/MonoDepth/splits/endovis/train_files.txt\n",
        "!cp -f /content/test_files.txt /content/MonoDepth/splits/endovis/test_files.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx0V47sEsvez",
        "outputId": "0c95772c-18c3-4c82-b8bb-112ef906bdc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python export_gt_depth.py --data_path /content/d3k1_tiffs --split endovis\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIYlvbePszRK",
        "outputId": "977cc65c-bdda-4429-baf0-e0e6377f0b90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "Exporting ground truth depths for endovis\n",
            "1\n",
            "dataset3/keyframe4\n",
            "2\n",
            "dataset3/keyframe4\n",
            "3\n",
            "dataset3/keyframe4\n",
            "4\n",
            "dataset3/keyframe4\n",
            "5\n",
            "dataset3/keyframe4\n",
            "6\n",
            "dataset3/keyframe4\n",
            "7\n",
            "dataset3/keyframe4\n",
            "8\n",
            "dataset3/keyframe4\n",
            "9\n",
            "dataset3/keyframe4\n",
            "10\n",
            "dataset3/keyframe4\n",
            "11\n",
            "dataset3/keyframe4\n",
            "12\n",
            "dataset3/keyframe4\n",
            "13\n",
            "dataset3/keyframe4\n",
            "14\n",
            "dataset3/keyframe4\n",
            "15\n",
            "dataset3/keyframe4\n",
            "16\n",
            "dataset3/keyframe4\n",
            "17\n",
            "dataset3/keyframe4\n",
            "18\n",
            "dataset3/keyframe4\n",
            "19\n",
            "dataset3/keyframe4\n",
            "20\n",
            "dataset3/keyframe4\n",
            "21\n",
            "dataset3/keyframe4\n",
            "22\n",
            "dataset3/keyframe4\n",
            "23\n",
            "dataset3/keyframe4\n",
            "24\n",
            "dataset3/keyframe4\n",
            "25\n",
            "dataset3/keyframe4\n",
            "26\n",
            "dataset3/keyframe4\n",
            "27\n",
            "dataset3/keyframe4\n",
            "28\n",
            "dataset3/keyframe4\n",
            "29\n",
            "dataset3/keyframe4\n",
            "30\n",
            "dataset3/keyframe4\n",
            "31\n",
            "dataset3/keyframe4\n",
            "32\n",
            "dataset3/keyframe4\n",
            "33\n",
            "dataset3/keyframe4\n",
            "34\n",
            "dataset3/keyframe4\n",
            "Saving to endovis\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python evaluate_depth.py --dpt --eval_split endovis --data_path /content/data --eval_mono  --width 320 --height 256 \n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXjigGU6s6R6",
        "outputId": "043de782-18a6-4ebd-e398-c9b32e20c6d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-09 16:49:16.334986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "MIN MAX of depth:  tensor(65.7045, device='cuda:0') tensor(307.5600, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(71.6708, device='cuda:0') tensor(284.8885, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(66.6071, device='cuda:0') tensor(286.6065, device='cuda:0')\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 11440.131 | std: 0.098\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.252  &   8.286  &  24.308  &   0.307  &   0.595  &   0.865  &   0.959  \\\\\n",
            "\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python test_simple.py --image_path assets/test_image.jpg --model_name mono_640x192\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V4LJR6qwWZK",
        "outputId": "99e425d3-86fc-495e-d924-bfaa11b3eeaa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-09 16:50:02.143388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Downloading pretrained model to models/mono_640x192.zip\n",
            "   Unzipping model...\n",
            "   Model unzipped to models/mono_640x192\n",
            "-> Loading model from  models/mono_640x192\n",
            "   Loading pretrained encoder\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "   Loading pretrained decoder\n",
            "-> Predicting on 1 test images\n",
            "   Processed 1 of 1 images - saved predictions to:\n",
            "   - assets/test_image_disp.jpeg\n",
            "   - assets/test_image_disp.npy\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python evaluate_depth.py  --eval_split endovis --data_path /content/data --eval_mono  --width 320 --height 256 --load_weights_folder /content/MonoDepth/models/mono_640x192 \n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZn82z6xEF18",
        "outputId": "a1b7c259-e35e-4e9a-c6b4-6d8c7b8561d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-09 16:50:27.434254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading weights from /content/MonoDepth/models/mono_640x192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "-> Computing predictions with size 640x192\n",
            "MIN MAX of depth:  tensor(1.0079, device='cuda:0') tensor(9.9664, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(1.1725, device='cuda:0') tensor(9.9894, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(1.1124, device='cuda:0') tensor(9.9837, device='cuda:0')\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 239.123 | std: 0.131\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.252  &   7.622  &  24.474  &   0.321  &   0.516  &   0.831  &   0.952  \\\\\n",
            "\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    }
  ]
}