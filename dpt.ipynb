{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgdan1+gj9mNnrRGjWUdC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsShi/Mono-Depth/blob/main/dpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/MonoDepth\n",
        "!git clone https://github.com/ItsShi/MonoDepth.git\n",
        "\n",
        "!pip -q install tensorboardX==1.4\n",
        "!pip -q install transformers\n",
        "!pip -q install torchvision==0.12.0"
      ],
      "metadata": {
        "id": "lFxFgv2BAuJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68055f4b-fdf7-4da0-c244-9f3f1c1f269b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MonoDepth'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 224 (delta 73), reused 106 (delta 56), pack-reused 84\u001b[K\n",
            "Receiving objects: 100% (224/224), 10.51 MiB | 32.42 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "d3k1_url = 'https://drive.google.com/uc?id=17k1-CHptG_XTXVX8qp7yK_zadH54vVnY'\n",
        "gdown.download(d3k1_url,'d3k1_rgb.mp4',quiet=True) "
      ],
      "metadata": {
        "id": "pQW2jxWsv61U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0531af87-e781-4df7-d9d2-a2513939cc63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'d3k1_rgb.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vid2frame\n",
        "!pip install ffmpeg\n",
        "!mkdir /content/data\n",
        "!ffmpeg -i /content/d3k1_rgb.mp4 %6d.jpg  \n",
        "!mv *.jpg /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TfwO3ResZ7B",
        "outputId": "682203ad-f98f-4481-d65b-f7823fd76c29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=3891bb3c2c3ea17d9dcaa13c3d965e885cfc40b4fbe15f9e5e0eb1fd99b0a265\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/d3k1_rgb.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf56.40.101\n",
            "  Duration: 00:00:13.16, start: 0.000000, bitrate: 31151 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x2048, 31148 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x56242f36cc40] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '%6d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0(und): Video: mjpeg, yuvj444p(pc), 1280x2048, q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      encoder         : Lavc58.54.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
            "frame=  329 fps= 14 q=24.8 Lsize=N/A time=00:00:13.16 bitrate=N/A speed=0.579x    \n",
            "video:39020kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# depth gt\n",
        "!pwd\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "scene_url = 'https://drive.google.com/uc?id=1xZwB0qFWbU7EjjI1g-Y0pqAs1_nWwU6Z'\n",
        "gdown.download(scene_url, 'scene.tar.gz', quiet=True)\n",
        "\n",
        "shutil.unpack_archive('scene.tar.gz', 'd3k1_tiffs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GYyfSKQlWJZ",
        "outputId": "daf103eb-bd80-494d-d5a9-094b454c33eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch train_files.txt\n",
        "!touch val_files.txt\n",
        "!touch test_files.txt\n",
        "\n",
        "for opt in [\"train\", \"val\", \"test\"]:\n",
        "    f = open(\"/content/MonoDepth/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if folder == \"dataset3/keyframe1\" or folder == \"dataset3/keyframe3\" or folder == \"dataset3/keyframe4\" and int(frame)<328:\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)\n",
        "\n",
        "# move and replace the original train_files.txt file\n",
        "%pwd\n",
        "!cp /content/MonoDepth/splits/endovis/test_files.txt /content/test_files_original.txt\n",
        "!cp /content/MonoDepth/splits/endovis/train_files.txt /content/train_files_original.txt\n",
        "!cp /content/MonoDepth/splits/endovis/val_files.txt /content/val_files_original.txt\n",
        "!cp -f /content/val_files.txt /content/MonoDepth/splits/endovis/val_files.txt\n",
        "!cp -f /content/train_files.txt /content/MonoDepth/splits/endovis/train_files.txt\n",
        "!cp -f /content/test_files.txt /content/MonoDepth/splits/endovis/test_files.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx0V47sEsvez",
        "outputId": "b86d886c-71f1-4c5b-9fba-1c7aa06d029f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "290\n",
            "284\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch train_files.txt\n",
        "!touch val_files.txt\n",
        "!touch test_files.txt\n",
        "\n",
        "for opt in [\"train\"]:\n",
        "    f = open(\"/content/MonoDepth/splits/endovis/{}_files.txt\".format(opt), \"r+\")\n",
        "    # f = open(\"/content/{}_files_original.txt\".format(opt), \"r+\")\n",
        "    new_file = []\n",
        "    for line in f:\n",
        "      folder, frame = line.split()[0], line.split()[1]\n",
        "      if int(folder[7]) < 8:\n",
        "          new_file.append(line)\n",
        "    print(len(new_file))\n",
        "    with open(\"{}_files.txt\".format(opt), \"w+\") as f:\n",
        "      for i in new_file:\n",
        "        f.write(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grmj2F9TFx0v",
        "outputId": "29805cd7-bb96-4135-dca1-0aa0b3124a2e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python test_simple.py --image_path assets/test_image.jpg --model_name mono_640x192\n",
        "%cd ..\n",
        "!cp -r MonoDepth/models/mono_640x192 mono_640x192"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcqKaRMHf_D",
        "outputId": "b38733c5-c610-4fb2-867a-00cce2ccba18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 16:05:20.318090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Downloading pretrained model to models/mono_640x192.zip\n",
            "   Unzipping model...\n",
            "   Model unzipped to models/mono_640x192\n",
            "-> Loading model from  models/mono_640x192\n",
            "   Loading pretrained encoder\n",
            "   Loading pretrained decoder\n",
            "-> Predicting on 1 test images\n",
            "   Processed 1 of 1 images - saved predictions to:\n",
            "   - assets/test_image_disp.jpeg\n",
            "   - assets/test_image_disp.npy\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python train.py --split endovis --data_path /content/data --dataset endovis --num_epochs 1 --log_frequency 20 --log_dir models --width 320 --height 256 --batch_size 4 --num_workers 0 --load_weights_folder /content/mono_640x192\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E_gF_ImIONA",
        "outputId": "4846b854-06cb-4d88-864f-e16349b04b8d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-12 04:40:24.328385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading model from folder /content/mono_640x192\n",
            "Loading encoder weights...\n",
            "Loading depth weights...\n",
            "Loading pose_encoder weights...\n",
            "Loading pose weights...\n",
            "Cannot find Adam weights so Adam is randomly initialized\n",
            "Training pose model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   models\n",
            "Training is using:\n",
            "   cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 290 training items and 284 validation items\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Training\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "epoch   0 | batch      0 | examples/s:  14.5 | loss: 0.05423 | time elapsed: 00h00m04s | time left: 00h00m00s\n",
            "epoch   0 | batch     20 | examples/s:  17.1 | loss: 0.02920 | time elapsed: 00h01m18s | time left: 00h03m23s\n",
            "epoch   0 | batch     40 | examples/s:  14.8 | loss: 0.02546 | time elapsed: 00h02m28s | time left: 00h01m58s\n",
            "epoch   0 | batch     60 | examples/s:  15.0 | loss: 0.01917 | time elapsed: 00h03m40s | time left: 00h00m44s\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train with cv weights, save weights to mdp\n",
        "%cd MonoDepth\n",
        "!python train.py --dpt --split endovis --data_path /content/data --dataset endovis  --num_epochs 1 --log_frequency 20 --log_dir models --model_name mdp --width 320 --height 256 --batch_size 4 --num_workers 0 --load_weights_folder /content/mono_640x192 --models_to_load \"pose_encoder\" \"pose\" --scales 0\n",
        "%cd .."
      ],
      "metadata": {
        "id": "Q7NVQlDrp5yg",
        "outputId": "38d17273-4003-4496-fa42-b7db58ad9897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 16:26:29.125132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading model from folder /content/mono_640x192\n",
            "Loading pose_encoder weights...\n",
            "Loading pose weights...\n",
            "Cannot find Adam weights so Adam is randomly initialized\n",
            "Training depth model named:\n",
            "   dpt\n",
            "Training pose model named:\n",
            "   mdp\n",
            "Models and tensorboard events files are saved to:\n",
            "   /content/MonoDepth/models\n",
            "Training is using:\n",
            "   cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 290 training items and 284 validation items\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Training\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "epoch   0 | batch      0 | examples/s:   1.8 | loss: 0.03332 | time elapsed: 00h00m04s | time left: 00h00m00s\n",
            "epoch   0 | batch     20 | examples/s:   1.9 | loss: 0.04088 | time elapsed: 00h02m00s | time left: 00h05m13s\n",
            "epoch   0 | batch     40 | examples/s:   1.9 | loss: 0.01497 | time elapsed: 00h03m55s | time left: 00h03m08s\n",
            "epoch   0 | batch     60 | examples/s:   1.9 | loss: 0.03703 | time elapsed: 00h05m46s | time left: 00h01m09s\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finetune on weights /content/MonoDepth/models/mdp/models/weights_0 \n",
        "%cd MonoDepth\n",
        "!python train.py --dpt --split endovis --data_path /content/data --dataset endovis  --num_epochs 1 --log_frequency 20 --log_dir models --model_name dpt --width 320 --height 256 --batch_size 4 --num_workers 0 --load_weights_folder /content/MonoDepth/models/mdp/models/weights_0 --models_to_load \"depth\" \"pose_encoder\" \"pose\" --scales 0\n",
        "%cd .."
      ],
      "metadata": {
        "id": "S4nf8IP7sHOS",
        "outputId": "3958c8b8-5a6f-4e23-f126-e37d32868554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 16:39:56.141509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading model from folder /content/MonoDepth/models/mdp/models/weights_0\n",
            "Loading depth weights...\n",
            "Loading pose_encoder weights...\n",
            "Loading pose weights...\n",
            "Loading Adam weights\n",
            "Training depth model named:\n",
            "   dpt\n",
            "Training pose model named:\n",
            "   dpt\n",
            "Models and tensorboard events files are saved to:\n",
            "   models\n",
            "Training is using:\n",
            "   cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "Using split:\n",
            "   endovis\n",
            "There are 290 training items and 284 validation items\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Training\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "epoch   0 | batch      0 | examples/s:   2.0 | loss: 0.03776 | time elapsed: 00h00m05s | time left: 00h00m00s\n",
            "epoch   0 | batch     20 | examples/s:   1.9 | loss: 0.01875 | time elapsed: 00h02m01s | time left: 00h05m16s\n",
            "epoch   0 | batch     40 | examples/s:   1.9 | loss: 0.02160 | time elapsed: 00h03m52s | time left: 00h03m06s\n",
            "epoch   0 | batch     60 | examples/s:   1.9 | loss: 0.02643 | time elapsed: 00h05m43s | time left: 00h01m08s\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python export_gt_depth.py --data_path /content/d3k1_tiffs --split endovis\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pd1L8izkPir",
        "outputId": "0fbc150d-b1df-43c0-ef33-b5f14d83f337"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "Exporting ground truth depths for endovis\n",
            "1\n",
            "dataset3/keyframe4\n",
            "2\n",
            "dataset3/keyframe4\n",
            "3\n",
            "dataset3/keyframe4\n",
            "4\n",
            "dataset3/keyframe4\n",
            "5\n",
            "dataset3/keyframe4\n",
            "6\n",
            "dataset3/keyframe4\n",
            "7\n",
            "dataset3/keyframe4\n",
            "8\n",
            "dataset3/keyframe4\n",
            "9\n",
            "dataset3/keyframe4\n",
            "10\n",
            "dataset3/keyframe4\n",
            "11\n",
            "dataset3/keyframe4\n",
            "12\n",
            "dataset3/keyframe4\n",
            "13\n",
            "dataset3/keyframe4\n",
            "14\n",
            "dataset3/keyframe4\n",
            "15\n",
            "dataset3/keyframe4\n",
            "16\n",
            "dataset3/keyframe4\n",
            "17\n",
            "dataset3/keyframe4\n",
            "18\n",
            "dataset3/keyframe4\n",
            "19\n",
            "dataset3/keyframe4\n",
            "20\n",
            "dataset3/keyframe4\n",
            "21\n",
            "dataset3/keyframe4\n",
            "22\n",
            "dataset3/keyframe4\n",
            "23\n",
            "dataset3/keyframe4\n",
            "24\n",
            "dataset3/keyframe4\n",
            "25\n",
            "dataset3/keyframe4\n",
            "26\n",
            "dataset3/keyframe4\n",
            "27\n",
            "dataset3/keyframe4\n",
            "28\n",
            "dataset3/keyframe4\n",
            "29\n",
            "dataset3/keyframe4\n",
            "30\n",
            "dataset3/keyframe4\n",
            "31\n",
            "dataset3/keyframe4\n",
            "32\n",
            "dataset3/keyframe4\n",
            "33\n",
            "dataset3/keyframe4\n",
            "34\n",
            "dataset3/keyframe4\n",
            "Saving to endovis\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python evaluate_depth.py --eval_split endovis --data_path /content/data --eval_mono  --width 320 --height 256 --load_weights_folder /content/mono_640x192\n",
        "%cd .."
      ],
      "metadata": {
        "id": "ZF0H0_RNu0YF",
        "outputId": "d07b8cc8-24ce-4826-8e00-6b4c3c0cc9fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 18:49:56.862303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading weights from /content/mono_640x192\n",
            "Dataset size HxW:  192 640\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "-> Computing predictions with size 640x192\n",
            "MIN MAX of depth:  tensor(1.0079, device='cuda:0') tensor(9.9664, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(1.1725, device='cuda:0') tensor(9.9894, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(1.1124, device='cuda:0') tensor(9.9837, device='cuda:0')\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 239.123 | std: 0.131\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.252  &   7.622  &  24.474  &   0.321  &   0.516  &   0.831  &   0.952  \\\\\n",
            "\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/MonoDepth/models/mdp/models/weights_0 \n",
        "# \n",
        "%cd MonoDepth\n",
        "!python evaluate_depth.py --dpt --eval_split endovis --data_path /content/data --eval_mono  --width 320 --height 256 --load_weights_folder /content/MonoDepth/models/mdp/models/weights_0 \n",
        "%cd .."
      ],
      "metadata": {
        "id": "dSyEDznMyfdq",
        "outputId": "5314782f-6200-4043-93f8-9b737218424f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 18:46:29.878097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading weights from /content/MonoDepth/models/mdp/models/weights_0\n",
            "Dataset size HxW:  256 320\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "-> Computing predictions with size 320x256\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 0.764 | std: 0.099\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.248  &   7.257  &  25.346  &   0.311  &   0.516  &   0.868  &   0.960  \\\\\n",
            "\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers\n",
        "!rm -rf /content/MonoDepth\n",
        "!git clone https://github.com/ItsShi/MonoDepth.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NES0Y7EoP7uX",
        "outputId": "37be8797-fc01-4d6f-f691-89b44067ea8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'MonoDepth'...\n",
            "remote: Enumerating objects: 221, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 221 (delta 72), reused 106 (delta 56), pack-reused 84\u001b[K\n",
            "Receiving objects: 100% (221/221), 10.51 MiB | 21.65 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1bgrGt-DM6xQ_eSkI8etMcV_SeyO2ZtMb'\n",
        "gdown.download(url,'dpt_weights_7.zip',quiet=True) \n",
        "!unzip -q dpt_weights_7.zip"
      ],
      "metadata": {
        "id": "5ECsnV_6P8TR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/weights_7\n",
        "%cd MonoDepth\n",
        "!python evaluate_depth.py --dpt --eval_split endovis --data_path /content/data --eval_mono  --width 320 --height 256 --load_weights_folder /content/weights_7\n",
        "%cd .."
      ],
      "metadata": {
        "id": "jrDcOLjCQKa8",
        "outputId": "7c315651-43a9-4103-888e-71aa8024289c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 18:55:45.092447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading weights from /content/weights_7\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "-> Computing predictions with size 320x256\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "MIN MAX of depth:  tensor(0.0100, device='cuda:0') tensor(0.0100, device='cuda:0')\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 0.764 | std: 0.099\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.248  &   7.257  &  25.346  &   0.311  &   0.516  &   0.868  &   0.960  \\\\\n",
            "\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "gt_url = 'https://drive.google.com/uc?id=1GHX4ML7kUP4xFZSi56VtpZSxZdwHa2oO'\n",
        "gdown.download(gt_url,'gt.zip',quiet=True) \n",
        "!unzip -q gt.zip"
      ],
      "metadata": {
        "id": "sYIg_Is9QCS_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/MonoDepth/models/dpt/models/weights_0 \n",
        "# /content/weights_7\n",
        "%cd MonoDepth\n",
        "!python test_simple.py --dpt --ext png --image_path /content/SCARED/d5k4_Left_Image.png --model_name /content/weights_7\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyrTBUk8QE6l",
        "outputId": "e4115081-df3d-46a8-ec94-49a66372b9eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 19:37:40.001232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading model from  /content/weights_7\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/dpt/feature_extraction_dpt.py:28: FutureWarning: The class DPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DPTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "   Loading pretrained decoder\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "-> Predicting on 1 test images\n",
            "torch.Size([1, 1, 1024, 1280])\n",
            "torch.Size([1, 1, 1024, 1280]) 1024 1280\n",
            "   Processed 1 of 1 images - saved predictions to:\n",
            "   - /content/SCARED/d5k4_Left_Image_disp.jpeg\n",
            "   - /content/SCARED/d5k4_Left_Image_disp.npy\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python test_simple.py --dpt --ext png --image_path /content/SCARED/d5k4_Left_Image.png \n",
        "%cd .."
      ],
      "metadata": {
        "id": "v0SMR4Hfa-_M",
        "outputId": "83d56955-85df-4a23-a879-2286b1e17719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 19:40:47.115297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MonoDepth/test_simple.py\", line 185, in <module>\n",
            "    test_simple(args)\n",
            "  File \"/content/MonoDepth/test_simple.py\", line 53, in test_simple\n",
            "    assert args.model_name is not None, \\\n",
            "AssertionError: You must specify the --model_name parameter; see README.md for an example\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MonoDepth\n",
        "!python test_simple.py --image_path assets/test_image.jpg --model_name mono_640x192\n",
        "%cd .."
      ],
      "metadata": {
        "id": "erqrZ_pYXnW5",
        "outputId": "c0dff2f5-b0a4-429f-9ba6-87c1b4b7f076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MonoDepth\n",
            "2023-06-14 19:27:40.757572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "-> Loading model from  models/mono_640x192\n",
            "   Loading pretrained encoder\n",
            "   Loading pretrained decoder\n",
            "-> Predicting on 1 test images\n",
            "torch.Size([1, 1, 192, 640]) 235 638\n",
            "   Processed 1 of 1 images - saved predictions to:\n",
            "   - assets/test_image_disp.jpeg\n",
            "   - assets/test_image_disp.npy\n",
            "-> Done!\n",
            "/content\n"
          ]
        }
      ]
    }
  ]
}